{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2TtFJBLOvq09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-06-12 20:23:53.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhcmus.core.appconfig\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoad DotEnv: True\u001b[0m\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from hcmus.core import appconfig\n",
        "from hcmus.lbs import LabelStudioConnector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "accepted_labels = \"\"\"\n",
        "8934804026817-nuoc-tuong-maggi-dau-nanh-thanh-diu-chai-700ml\n",
        "8934804020402-nuoc-tuong-maggi-dau-nanh-dam-dac-700ml\n",
        "8936048470524-mi-tom-3-mien-gold-vi-bo-ham-rau-thom-75g\n",
        "8934822201272-bia-heineken-silver-lon-250ml\n",
        "5054563022592-kem-danh-rang-sensodyne-giam-e-buotlam-sang-rang-100g\n",
        "8934868166344-nuoc-rua-tay-lifebouy-450ml\n",
        "8934707029182-hat-nem-knorr-thit-than-xuong-ong-va-tuy-900g\n",
        "8934822361211-bia-heineken-khong-con-lon-330ml\n",
        "8850006932346-kem-danh-rang-colgate-maxfresh-huong-tra-xanh-225g-+-ban-chai-long-to\n",
        "8850006932322-kem-danh-rang-colgate-maxfresh-huong-bac-ha-225g-+-ban-chai-long-to\n",
        "6920354827792-kem-danh-rang-colgate-maxfresh-tinh-chat-than-tre-225g-+-ban-chai-long-to\n",
        "8936136163314-nuoc-mam-nam-ngu-3-trong-1-750ml\n",
        "8934563198152-mi-hao-hao-chay-vi-rau-nam-74gr\n",
        "8935049501503-nuoc-ngot-coca-cola-vi-nguyen-ban-lon-320ml\n",
        "8936048470036-mi-3-mien-mi-chay-la-da-65g-(goi)\n",
        "8935001215028-tra-tea-break-milk-350ml\n",
        "8935006532755-kem-rua-mat-acnes-vitamin-lam-sang-da,-mo-seo-tham-50g\n",
        "8935018801641-khan-bep-da-nang-pulppy-classic-2-lop-loc-2-cuon\n",
        "8935024123287-ca-phe-sua-hoa-tan-g7-3in1-hop-336g-(16g-x-21-goi)\n",
        "8935006538634-dau-tay-trang-hada-labo-sach-sau-duong-am-toi-uu-200ml\n",
        "8934840000093-nuoc-khoang-chanh-lemona-chai-500ml\n",
        "8997035601321-nuoc-uong-bu-dien-giai-bo-sung-ion-pocari-sweat-chai-900ml\n",
        "4897036693162-nuoc-tang-luc-monster-mango-loco-355ml\n",
        "809939073648-soda-sua-huong-vi-chuoi-milkis-banana\n",
        "8934804038131-tra-hoa-qua-nestea-hoa-tan-hop-144g-12-goi-x-12g\n",
        "8936048471248-mi-tom-chua-cay-dac-biet-3-mien-gold-goi-75g\n",
        "8934804027333-tra-chanh-nestea-hoa-tan-hop-195g-15-goi-x-13g\n",
        "8936010530881-tra-chanh-cozy-hoa-tan-hop-240g-16-goi-x-15g\n",
        "8935049510604-nuoc-ngot-sprite-chai-320ml\n",
        "8934588672118-nuoc-ngot-pepsi-khong-calo-vi-chanh-320ml\n",
        "8935049501572-nuoc-ngot-fanta-vi-cam-lon-320ml\n",
        "8935049500445-nuoc-ngot-fanta-chai-1.5l\n",
        "8936721790017-nuoc-loc-th-true-water-chai-500ml\n",
        "8935049501718-nuoc-ngot-sprite-lon-320ml\n",
        "8936193070013-tra-xanh-khong-do-500ml\n",
        "8935137601122-tra-xanh-thai-nguyen-100g\n",
        "8934564600883-tra-den-c2-vi-dua-luoi-chai-455ml\n",
        "8935001214274-tra-bi-dao-wonderfarm-lon-310ml\n",
        "8934588870552-tra-olong-teaplus-450ml\n",
        "6920354836930-kem-danh-rang-colgate-optic-white-cong-nghe-sac-tim-100g\n",
        "8935049501374-nuoc-ngot-cocacola-vi-nguyen-ban-chai-600ml\n",
        "8935039570700-ca-phe-sua-robusta-birdy-lon-170ml\n",
        "8934673701402-nuoc-dao-necta-vinamilk-hop-1l\n",
        "8935005801012-nuoc-loc-lavie-chai-1-5l\n",
        "8936079140021-ca-phe-sua-highland-coffee-lon-185ml\n",
        "8934588640445-ca-phe-sua-boss-lon-180ml\n",
        "8934683008614-ca-phe-sua-vinacafe-3-in-1-gold-tui-480g-24-goi-tui\n",
        "8934683008867-nuoc-tang-luc-wake-up-247-vi-ca-phe-330ml\n",
        "8935328600156-tra-xanh-tui-loc-phuc-long-2g-x-hop-25-goi\n",
        "4902430805322-nuoc-rua-tay-diet-khuan-safeguard-trang-tinh-khiet-450ml\n",
        "8936156730718-nuoc-xa-vai-blue-dam-dac-huong-thanh-xuan-tui-3.2l\n",
        "8934868166825-dau-goi-sunsilk-natural-duong-ngan-gay-rung-650g\n",
        "8934804040523-tra-vai-va-huong-hoa-lai-nestea-hoa-tan-hop-144g-12-goi-x-12g\n",
        "8934822801335-bia-tiger-crystal-4-6-phan-tram-lon-330ml\n",
        "8934868156871-dau-xa-sunsilk-mem-muot-dieu-ky-320g\n",
        "8936010530713-tra-dao-cozy-hoa-tan-hop-16-goi-x-15g\n",
        "8934868171850-nuoc-lau-kinh-sunlight-sieu-nhanh-sach-trong-suot-chai-520ml\n",
        "8850228003541-nuoc-tang-luc-warrior-huong-dau-chai-330ml\n",
        "8936193070075-nuoc-tang-luc-number-1-chai-330ml\n",
        "8934588012112-nuoc-ngot-pepsi-cola-320ml\n",
        "8936013254951-nuoc-rua-kinh-gift-540ml\n",
        "8934868190394-nuoc-giat-comfort-huong-hoa-anh-dao-va-dao-tuoi-tui-3kg\n",
        "3068320055008-nuoc-loc-evivan-chai-500ml\n",
        "8936127794206-nuoc-uong-sua-trai-cay-th-true-juice-milk-huong-viet-quat-300ml\n",
        "8935049511038-sua-trai-cay-nutriboost-cam-chai-1l\n",
        "8934673573399-sua-tuoi-tiet-trung-co-duong,-100%-sua-tuoivinamilk-(1lit).\n",
        "3760128640607-sua-tuoi-tiet-trung-promess-nguyen-kem-hop-1l\n",
        "8934822212339-bia-heineken-lon-cao-330ml\n",
        "8801382123446-nuoc-gao-han-quoc-loai-500ml\n",
        "8936036201604-bia-hoegaarden-rosee-chai-248ml\n",
        "8934692090013-bia-1664-blanc-5%-abv-chai-330ml\n",
        "8934868166870-dau-goi-sunsilk-ong-muot-rang-ngoi-chai-650g\n",
        "8934588233074-nuoc-tang-luc-sting-vi-dau-tay-do-330ml\n",
        "8934822431211-bia-tiger-soju-infused-lager-wonder-melon-vi-dua-luoi-lon-330ml\n",
        "8934822112332-bia-tiger-lon-cao-330ml-lon\n",
        "8934868180470-dau-goi-dove-biotin-ngan-gay-rung-toc-880g\n",
        "8935024170519-ca-phe-trung-nguyen-legend-classic-hop-357g-17g-x-21-goi\n",
        "8934868166351-nuoc-rua-tay-lifebuoy-matcha-va-lo-hoichai-450g\n",
        "8936191270057-ca-phe-cappuccino-dua-just-viet-hop-hop-10-goi-x-17g\n",
        "8934868166924-dau-goi-sunsilk-mem-muot-dieu-ky-chai-650g\n",
        "8934804028064-ca-phe-hoa-tan-nescafe-3in1-vi-nguyen-ban-tui-782g-(17g-x-46-goi)\n",
        "8936122200276-ca-phe-mr.brown-blue-mountain-240ml\n",
        "8934563122201-mi-hao-hao-vi-tom-chua-cay-goi-100g\n",
        "8935136865709-sua-tam-purite-duong-da-mem-min-huong-hoa-hong-rose-chai-850ml\n",
        "8934822451295-nuoc-trai-cay-len-men-strongbow-vi-thom-luu-3.5%-lon-330ml\n",
        "8934868170990-nuoc-rua-tay-lifebuoy-vitamin-sua-duong-am-tui-1kg\n",
        "8936094291005-bia-budweiser-5-lon-330ml\n",
        "8934822121297-nuoc-trai-cay-len-men-strongbow-red-beries-4.5%-abv-lon-320ml\n",
        "8934822111298-nuoc-trai-cay-len-men-strongbow-gold-apple-4-5-abv-lon-320ml\n",
        "8935049501190-nuoc-tra-fuze-tea-vi-chanh-sa-450ml\n",
        "8936199810026-cafe-the-coffee-house-sua-da-22gx10-goi-(hop)\n",
        "8888589308920-tra-bi-dao-winter-melon-jj-lon-300ml\n",
        "8934673101097-sua-tuoi-tiet-trung-tiet-trung-vinamilk-khong-duong-bich-220ml\n",
        "8934563183158-mi-hao-hao-sa-te-hanh-tim-75g\n",
        "8936048470012-mi-3-mien-tom-chua-cay-65g\n",
        "8934683009925-ca-phe-sua-da-vinacafe-chat-hop-10-goi-24g\n",
        "8850228007617-nuoc-tang-luc-redbull-250ml\n",
        "8934563651138-mi-hao-hao,-vi-tom-chua-cayacecook,-coc-(67g)\n",
        "8934588063060-nuoc-loc-aquafina-chai-1-5l\n",
        "8935049510857-nuoc-loc-dasani-chai-1.5l\"\"\"\n",
        "accepted_labels = accepted_labels.splitlines()\n",
        "accepted_labels = [x for x in accepted_labels if len(x) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HJ-srbQqvtn8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-06-12 20:23:56.967\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mhcmus.lbs._label_studio_connector\u001b[0m:\u001b[36mget_tasks\u001b[0m:\u001b[36m144\u001b[0m - \u001b[33m\u001b[1mPage size is too large, only 3223 tasks available.\u001b[0m\n",
            "\u001b[32m2025-06-12 20:23:56.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhcmus.lbs._label_studio_connector\u001b[0m:\u001b[36mget_tasks\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mNew `page_to` applied: 33\u001b[0m\n",
            "Loading tasks: 100%|██████████| 33/33 [00:09<00:00,  3.47it/s]\n",
            "Downloading images: 100%|██████████| 3223/3223 [00:05<00:00, 605.66it/s] \n",
            "\u001b[32m2025-06-12 20:24:11.973\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mhcmus.lbs._label_studio_connector\u001b[0m:\u001b[36mget_tasks\u001b[0m:\u001b[36m144\u001b[0m - \u001b[33m\u001b[1mPage size is too large, only 378 tasks available.\u001b[0m\n",
            "\u001b[32m2025-06-12 20:24:11.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhcmus.lbs._label_studio_connector\u001b[0m:\u001b[36mget_tasks\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mNew `page_to` applied: 4\u001b[0m\n",
            "Loading tasks: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]\n",
            "Downloading images: 100%|██████████| 378/378 [00:00<00:00, 1341.71it/s]\n",
            "\u001b[32m2025-06-12 20:24:15.749\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mhcmus.lbs._label_studio_connector\u001b[0m:\u001b[36mget_tasks\u001b[0m:\u001b[36m144\u001b[0m - \u001b[33m\u001b[1mPage size is too large, only 30 tasks available.\u001b[0m\n",
            "\u001b[32m2025-06-12 20:24:15.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhcmus.lbs._label_studio_connector\u001b[0m:\u001b[36mget_tasks\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1mNew `page_to` applied: 1\u001b[0m\n",
            "Loading tasks: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
            "Downloading images: 100%|██████████| 30/30 [00:00<00:00, 6217.47it/s]\n"
          ]
        }
      ],
      "source": [
        "splits = {}\n",
        "for split_name in [\"train\", \"test\", \"val\"]:\n",
        "    lsb_connector = LabelStudioConnector(\n",
        "        url=appconfig.LABEL_STUDIO_URL,\n",
        "        api_key=appconfig.LABEL_STUDIO_API_KEY,\n",
        "        project_id=appconfig.LABEL_STUDIO_PROJECT_MAPPING[split_name],\n",
        "        temp_dir=appconfig.LABEL_STUDIO_TEMP_DIR\n",
        "    )\n",
        "\n",
        "    tasks = lsb_connector.get_tasks()\n",
        "    labels = lsb_connector.extract_labels(tasks)\n",
        "    dataset = lsb_connector.download_dataset(tasks, labels)\n",
        "    dataset = lsb_connector.transform_labels(dataset, labels, accepted_labels)\n",
        "    splits[split_name] = dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn import functional as F\n",
        "from loguru import logger\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from hcmus.data import CroppedObjectClassificationDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "common_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    # transforms.RandomCrop(size=(224, 224)),\n",
        "\n",
        "    # transforms.RandomResizedCrop(\n",
        "    #     size=(224, 224),\n",
        "    #     scale=(0.8, 1.0),\n",
        "    #     ratio=(0.9, 1.1)\n",
        "    # ),\n",
        "\n",
        "    # Randomly change brightness, contrast, saturation, and hue\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "\n",
        "    # Random horizontal and vertical flip\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomVerticalFlip(p=1),\n",
        "\n",
        "    # Random rotation within ±30 degrees\n",
        "    transforms.RandomRotation(degrees=90),\n",
        "\n",
        "    # Random affine transformation (rotation, translation, scale, shear)\n",
        "    transforms.RandomAffine(\n",
        "        degrees=15,          # additional rotation control\n",
        "        translate=(0.1, 0.1),  # 5% translation in both directions\n",
        "        scale=(0.8, 1.1),    # zoom in/out\n",
        "        shear=20             # shear angle\n",
        "    ),\n",
        "\n",
        "    # Random perspective transformation\n",
        "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
        "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = []\n",
        "for x in splits[\"train\"]:\n",
        "    labels.extend(x.get(\"target\").get(\"labels\"))\n",
        "labels = set(labels)\n",
        "len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-06-12 20:24:16.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhcmus.data._torch_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mAuto infer `label2idx` mapping, mapping length: 99.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_NAMES = [\n",
        "    # 'resnet50',\n",
        "    'densenet121'\n",
        "]\n",
        "LR = 1e-3\n",
        "ARC_EMB_SIZE = 512\n",
        "# NUM_CLASSES = len(accepted_labels) + 1\n",
        "EPOCHS = 128\n",
        "train_dataset = CroppedObjectClassificationDataset(splits[\"train\"], transforms=common_transforms, skip_labels=[\"object\"])\n",
        "val_dataset = CroppedObjectClassificationDataset(splits[\"val\"], label2idx=train_dataset.label2idx, transforms=transform_val, skip_labels=[\"object\"])\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "NUM_CLASSES = len(train_dataset.label2idx)\n",
        "print(NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "experiment_name = \"/Classifier\"\n",
        "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "experiment_id = None\n",
        "if not experiment:\n",
        "    experiment_id = mlflow.create_experiment(experiment_name)\n",
        "else:\n",
        "    experiment_id = experiment.experiment_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-06-12 20:24:17.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m149\u001b[0m - \u001b[1m\n",
            "🔍 Training with frozen backbone: densenet121\u001b[0m\n",
            "\u001b[32m2025-06-12 20:24:18.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mEpoch 1/128\u001b[0m\n",
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]        /Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-06-12 20:29:06.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mTrain Loss: 16.3061, Train Acc: 40.06%, Val Acc: 50.61%\u001b[0m\n",
            "\u001b[32m2025-06-12 20:29:06.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mEpoch 2/128\u001b[0m\n",
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]        /Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-06-12 20:33:33.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mTrain Loss: 12.6559, Train Acc: 68.64%, Val Acc: 65.24%\u001b[0m\n",
            "\u001b[32m2025-06-12 20:33:33.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mEpoch 3/128\u001b[0m\n",
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]        /Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-06-12 20:38:09.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mTrain Loss: 11.0298, Train Acc: 77.67%, Val Acc: 62.80%\u001b[0m\n",
            "\u001b[32m2025-06-12 20:38:09.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mEpoch 4/128\u001b[0m\n",
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]        /Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-06-12 20:42:42.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mTrain Loss: 9.8527, Train Acc: 81.76%, Val Acc: 70.12%\u001b[0m\n",
            "\u001b[32m2025-06-12 20:42:42.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mEpoch 5/128\u001b[0m\n",
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]        /Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-06-12 20:47:16.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mTrain Loss: 9.1205, Train Acc: 84.28%, Val Acc: 71.34%\u001b[0m\n",
            "\u001b[32m2025-06-12 20:47:16.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mEpoch 6/128\u001b[0m\n",
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]        /Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-06-12 20:51:53.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mTrain Loss: 8.4134, Train Acc: 86.81%, Val Acc: 63.41%\u001b[0m\n",
            "\u001b[32m2025-06-12 20:51:53.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mEpoch 7/128\u001b[0m\n",
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]        /Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run arc-densenet121 at: http://jimica.ddns.net:5050/#/experiments/691937946546268522/runs/752dd9e1873248479378f34b927c699a\n",
            "🧪 View experiment at: http://jimica.ddns.net:5050/#/experiments/691937946546268522\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 167\u001b[0m\n\u001b[1;32m    165\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_model(model, criterion, optimizer)\n\u001b[0;32m--> 167\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_with_criterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_metrics({\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_acc,\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_acc\n\u001b[1;32m    174\u001b[0m }, step\u001b[38;5;241m=\u001b[39mepoch)\n",
            "Cell \u001b[0;32mIn[10], line 133\u001b[0m, in \u001b[0;36mevaluate_model_with_criterion\u001b[0;34m(model, criterion)\u001b[0m\n\u001b[1;32m    130\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, ArcModel):\n\u001b[0;32m--> 133\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Use the same weight matrix from criterion for evaluation\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     W_norm \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(criterion\u001b[38;5;241m.\u001b[39mW, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[10], line 92\u001b[0m, in \u001b[0;36mArcModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 92\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     94\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/timm/models/densenet.py:306\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 306\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/timm/models/densenet.py:298\u001b[0m, in \u001b[0;36mDenseNet.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/timm/models/densenet.py:123\u001b[0m, in \u001b[0;36mDenseBlock.forward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m    121\u001b[0m features \u001b[38;5;241m=\u001b[39m [init_features]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 123\u001b[0m     new_features \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(new_features)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, \u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/timm/models/densenet.py:87\u001b[0m, in \u001b[0;36mDenseLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_checkpoint_bottleneck(prev_features)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottleneck_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m new_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(bottleneck_output))\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/timm/models/densenet.py:45\u001b[0m, in \u001b[0;36mDenseLayer.bottleneck_fn\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbottleneck_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# type: (List[torch.Tensor]) -> torch.Tensor\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     concated_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(xs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcated_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: T484\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bottleneck_output\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training and evaluation functions\n",
        "def train_model(model, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        # images, labels = cutmix(images, labels)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if isinstance(model, ArcModel):\n",
        "            # For ArcFace model, get embeddings and use ArcFace loss\n",
        "            embeddings = model(images)\n",
        "            loss = criterion(embeddings, labels)\n",
        "            # For accuracy calculation, we need to compute cosine similarity\n",
        "            with torch.no_grad():\n",
        "                embeddings_norm = F.normalize(embeddings, dim=1)\n",
        "                W_norm = F.normalize(criterion.W, dim=0)\n",
        "                logits = torch.matmul(embeddings_norm, W_norm) * criterion.s\n",
        "                _, predicted = logits.max(1)\n",
        "        else:\n",
        "            # For regular classification model\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "def evaluate_model(model):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            if isinstance(model, ArcModel):\n",
        "                # For ArcFace model, get embeddings and compute similarity\n",
        "                embeddings = model(images)\n",
        "                embeddings_norm = F.normalize(embeddings, dim=1)\n",
        "                # Access the ArcFace loss layer to get weights\n",
        "                # Assuming criterion is accessible here, otherwise pass it as parameter\n",
        "                # For now, let's compute it directly in the model\n",
        "                W_norm = F.normalize(model.classifier.W, dim=0) if hasattr(model, 'classifier') else None\n",
        "                if W_norm is not None:\n",
        "                    logits = torch.matmul(embeddings_norm, W_norm)\n",
        "                    _, predicted = logits.max(1)\n",
        "                else:\n",
        "                    # Fallback: add a classification head for evaluation\n",
        "                    if not hasattr(model, 'eval_classifier'):\n",
        "                        model.eval_classifier = nn.Linear(embeddings.size(1), NUM_CLASSES).to(DEVICE)\n",
        "                    logits = model.eval_classifier(embeddings)\n",
        "                    _, predicted = logits.max(1)\n",
        "            else:\n",
        "                # For regular classification model\n",
        "                outputs = model(images)\n",
        "                _, predicted = outputs.max(1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    return 100. * correct / total\n",
        "\n",
        "# Freeze backbone parameters\n",
        "def freeze_backbone(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Build model with frozen backbone + classifier head\n",
        "def build_feature_extractor(model_name):\n",
        "    backbone = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool='avg')\n",
        "    freeze_backbone(backbone)\n",
        "    model = nn.Sequential(\n",
        "        backbone,\n",
        "        nn.Linear(backbone.num_features, NUM_CLASSES)\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "class ArcModel(nn.Module):\n",
        "    def __init__(self, backbone_name, embedding_size=512):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool='avg')\n",
        "        freeze_backbone(self.backbone)\n",
        "        self.embedding = nn.Linear(self.backbone.num_features, embedding_size)\n",
        "        self.bn = nn.BatchNorm1d(embedding_size)\n",
        "        # Add classification weights for evaluation\n",
        "        self.classifier = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.embedding(x)\n",
        "        x = self.bn(x)\n",
        "        return F.normalize(x, dim=1)  # Return normalized embeddings\n",
        "\n",
        "class ArcFaceLoss(nn.Module):\n",
        "    def __init__(self, embedding_size, num_classes, s=30.0, m=0.50):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(embedding_size, num_classes))\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        # Embeddings should already be normalized from model\n",
        "        W = F.normalize(self.W, dim=0)\n",
        "        cosine = torch.matmul(embeddings, W)  # [B, C]\n",
        "\n",
        "        # Clamp cosine values to prevent numerical issues\n",
        "        cosine = torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7)\n",
        "        theta = torch.acos(cosine)\n",
        "        target_logits = torch.cos(theta + self.m)\n",
        "\n",
        "        # Create one-hot encoding\n",
        "        one_hot = F.one_hot(labels, num_classes=self.num_classes).float().to(embeddings.device)\n",
        "\n",
        "        # Apply margin only to target class\n",
        "        logits = self.s * (one_hot * target_logits + (1 - one_hot) * cosine)\n",
        "\n",
        "        return F.cross_entropy(logits, labels)\n",
        "\n",
        "# Better evaluation function that works with ArcFace\n",
        "def evaluate_model_with_criterion(model, criterion):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            if isinstance(model, ArcModel):\n",
        "                embeddings = model(images)\n",
        "                # Use the same weight matrix from criterion for evaluation\n",
        "                W_norm = F.normalize(criterion.W, dim=0)\n",
        "                logits = torch.matmul(embeddings, W_norm) * criterion.s\n",
        "                _, predicted = logits.max(1)\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "                _, predicted = outputs.max(1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    return 100. * correct / total\n",
        "\n",
        "# Training loop\n",
        "for model_name in MODEL_NAMES:\n",
        "    with mlflow.start_run(experiment_id=experiment_id, run_name=\"arc-\" + model_name):\n",
        "        logger.info(f\"\\n🔍 Training with frozen backbone: {model_name}\")\n",
        "\n",
        "        model = ArcModel(model_name, embedding_size=ARC_EMB_SIZE)\n",
        "        criterion = ArcFaceLoss(embedding_size=ARC_EMB_SIZE, num_classes=NUM_CLASSES)\n",
        "\n",
        "        # Only train the new layers (embedding layer and batch norm)\n",
        "        trainable_params = []\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'backbone' not in name:  # Don't train backbone\n",
        "                trainable_params.append(param)\n",
        "        trainable_params.extend(criterion.parameters())  # Include ArcFace weights\n",
        "\n",
        "        optimizer = torch.optim.Adam(trainable_params, lr=LR)\n",
        "\n",
        "        best_acc = 0\n",
        "        for epoch in range(EPOCHS):\n",
        "            logger.info(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "            train_loss, train_acc = train_model(model, criterion, optimizer)\n",
        "            val_acc = evaluate_model_with_criterion(model, criterion)\n",
        "\n",
        "            logger.info(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "            mlflow.log_metrics({\n",
        "                \"train_accuracy\": train_acc,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"val_accuracy\": val_acc\n",
        "            }, step=epoch)\n",
        "\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                # Save both model and criterion for complete model\n",
        "                # mlflow.pytorch.log_model({\n",
        "                #     'model': model,\n",
        "                #     'criterion': criterion\n",
        "                # }, \"model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Volumes/Cucumber/Projects/item-identification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "images, labels = next(iter(val_loader))\n",
        "images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "outputs = model(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
