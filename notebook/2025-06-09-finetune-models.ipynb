{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 16:09:31.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhcmus.core.appconfig\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mLoad DotEnv: True\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from hcmus.core import appconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Dataset Merger and Multi-Model Training Pipeline\n",
    "# Compatible with Kaggle environment\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import detection\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class COCODatasetMerger:\n",
    "    \"\"\"Merge two COCO datasets with custom weights\"\"\"\n",
    "\n",
    "    def __init__(self, dataset1_path, dataset2_path, output_path, weight1=0.5, weight2=0.5):\n",
    "        self.dataset1_path = Path(dataset1_path)\n",
    "        self.dataset2_path = Path(dataset2_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        self.weight1 = weight1\n",
    "        self.weight2 = weight2\n",
    "        self.num_classes = 0  # Will be set during merging\n",
    "        self.merged_categories = []  # Will store merged categories\n",
    "\n",
    "        # Ensure weights sum to 1\n",
    "        total_weight = weight1 + weight2\n",
    "        self.weight1 = weight1 / total_weight\n",
    "        self.weight2 = weight2 / total_weight\n",
    "\n",
    "    def load_coco_annotation(self, ann_path):\n",
    "        \"\"\"Load COCO annotation file\"\"\"\n",
    "        with open(ann_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def merge_datasets(self):\n",
    "        \"\"\"Merge two COCO datasets with custom weights\"\"\"\n",
    "        logger.info(\"Starting dataset merging...\")\n",
    "\n",
    "        # First, analyze categories from both datasets to determine number of classes\n",
    "        self._analyze_categories()\n",
    "\n",
    "        # Create output directory structure\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            (self.output_path / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Process each split\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            self._merge_split(split)\n",
    "\n",
    "        logger.info(\"Dataset merging completed!\")\n",
    "        logger.info(f\"Total number of classes: {self.num_classes}\")\n",
    "        logger.info(f\"Categories: {[cat['name'] for cat in self.merged_categories]}\")\n",
    "\n",
    "        return self.num_classes, self.merged_categories\n",
    "\n",
    "    def _analyze_categories(self):\n",
    "        \"\"\"Analyze categories from both datasets to determine total number of classes\"\"\"\n",
    "        logger.info(\"Analyzing categories from both datasets...\")\n",
    "\n",
    "        all_categories = []\n",
    "\n",
    "        # Get categories from both datasets by checking any available split\n",
    "        for dataset_path in [self.dataset1_path, self.dataset2_path]:\n",
    "            categories_found = False\n",
    "            for split in ['train', 'val', 'test']:\n",
    "                ann_path = dataset_path / split / f'annotations_{split}.json'\n",
    "                if ann_path.exists():\n",
    "                    ann_data = self.load_coco_annotation(ann_path)\n",
    "                    if 'categories' in ann_data and ann_data['categories']:\n",
    "                        all_categories.extend(ann_data['categories'])\n",
    "                        categories_found = True\n",
    "                        break\n",
    "\n",
    "            if not categories_found:\n",
    "                logger.warning(f\"No categories found in dataset: {dataset_path}\")\n",
    "\n",
    "        # Merge categories and remove duplicates\n",
    "        self.merged_categories = self._merge_categories_list(all_categories)\n",
    "        self.num_classes = len(self.merged_categories) + 1  # +1 for background class\n",
    "\n",
    "        logger.info(f\"Found {len(self.merged_categories)} unique categories\")\n",
    "        logger.info(f\"Total classes (including background): {self.num_classes}\")\n",
    "\n",
    "    def _merge_categories_list(self, all_categories):\n",
    "        \"\"\"Merge category lists from multiple sources, removing duplicates\"\"\"\n",
    "        merged_cats = []\n",
    "        seen_names = set()\n",
    "\n",
    "        for cat in all_categories:\n",
    "            if cat['name'] not in seen_names:\n",
    "                merged_cats.append(cat.copy())\n",
    "                seen_names.add(cat['name'])\n",
    "\n",
    "        # Reassign IDs to be sequential starting from 1\n",
    "        for i, cat in enumerate(merged_cats):\n",
    "            cat['id'] = i + 1\n",
    "\n",
    "        return merged_cats\n",
    "        \"\"\"Merge a specific split (train/val/test)\"\"\"\n",
    "        logger.info(f\"Merging {split} split...\")\n",
    "\n",
    "        # Load annotations\n",
    "        ann1_path = self.dataset1_path / split / f'annotations_{split}.json'\n",
    "        ann2_path = self.dataset2_path / split / f'annotations_{split}.json'\n",
    "\n",
    "        if not ann1_path.exists() or not ann2_path.exists():\n",
    "            logger.warning(f\"Skipping {split} split - annotation files not found\")\n",
    "            return\n",
    "\n",
    "        ann1 = self.load_coco_annotation(ann1_path)\n",
    "        ann2 = self.load_coco_annotation(ann2_path)\n",
    "\n",
    "        # Calculate number of samples based on weights\n",
    "        total_samples1 = len(ann1['images'])\n",
    "        total_samples2 = len(ann2['images'])\n",
    "\n",
    "        n_samples1 = int(total_samples1 * self.weight1 / (self.weight1 + self.weight2 * total_samples2 / total_samples1))\n",
    "        n_samples2 = int(total_samples2 * self.weight2 / (self.weight2 + self.weight1 * total_samples1 / total_samples2))\n",
    "\n",
    "        # Sample images\n",
    "        sampled_imgs1 = random.sample(ann1['images'], min(n_samples1, total_samples1))\n",
    "        sampled_imgs2 = random.sample(ann2['images'], min(n_samples2, total_samples2))\n",
    "\n",
    "    def _merge_split(self, split):\n",
    "        \"\"\"Merge a specific split (train/val/test)\"\"\"\n",
    "        logger.info(f\"Merging {split} split...\")\n",
    "\n",
    "        # Load annotations\n",
    "        ann1_path = self.dataset1_path / split / f'annotations_{split}.json'\n",
    "        ann2_path = self.dataset2_path / split / f'annotations_{split}.json'\n",
    "\n",
    "        if not ann1_path.exists() and not ann2_path.exists():\n",
    "            logger.warning(f\"Skipping {split} split - no annotation files found\")\n",
    "            return\n",
    "\n",
    "        # Load available annotations\n",
    "        ann1 = self.load_coco_annotation(ann1_path) if ann1_path.exists() else {'images': [], 'annotations': []}\n",
    "        ann2 = self.load_coco_annotation(ann2_path) if ann2_path.exists() else {'images': [], 'annotations': []}\n",
    "\n",
    "        # Calculate number of samples based on weights\n",
    "        total_samples1 = len(ann1['images'])\n",
    "        total_samples2 = len(ann2['images'])\n",
    "\n",
    "        if total_samples1 == 0 and total_samples2 == 0:\n",
    "            logger.warning(f\"No images found in {split} split\")\n",
    "            return\n",
    "\n",
    "        # Calculate samples to include based on weights\n",
    "        if total_samples1 > 0 and total_samples2 > 0:\n",
    "            n_samples1 = max(1, int(total_samples1 * self.weight1))\n",
    "            n_samples2 = max(1, int(total_samples2 * self.weight2))\n",
    "        else:\n",
    "            n_samples1 = total_samples1\n",
    "            n_samples2 = total_samples2\n",
    "\n",
    "        # Sample images\n",
    "        sampled_imgs1 = random.sample(ann1['images'], min(n_samples1, total_samples1)) if total_samples1 > 0 else []\n",
    "        sampled_imgs2 = random.sample(ann2['images'], min(n_samples2, total_samples2)) if total_samples2 > 0 else []\n",
    "\n",
    "        # Create merged annotation using pre-analyzed categories\n",
    "        merged_ann = {\n",
    "            'info': ann1.get('info', {}) if ann1_path.exists() else ann2.get('info', {}),\n",
    "            'licenses': ann1.get('licenses', []) if ann1_path.exists() else ann2.get('licenses', []),\n",
    "            'categories': self.merged_categories,  # Use pre-analyzed categories\n",
    "            'images': [],\n",
    "            'annotations': []\n",
    "        }\n",
    "\n",
    "        # Create category ID mapping for both datasets\n",
    "        cat_id_map1 = self._create_category_mapping(ann1.get('categories', []))\n",
    "        cat_id_map2 = self._create_category_mapping(ann2.get('categories', []))\n",
    "\n",
    "        # Process dataset 1\n",
    "        img_id_mapping1 = {}\n",
    "        ann_id_counter = 1\n",
    "\n",
    "        for img in sampled_imgs1:\n",
    "            new_img_id = len(merged_ann['images']) + 1\n",
    "            img_id_mapping1[img['id']] = new_img_id\n",
    "\n",
    "            new_img = img.copy()\n",
    "            new_img['id'] = new_img_id\n",
    "            merged_ann['images'].append(new_img)\n",
    "\n",
    "            # Copy image file\n",
    "            src_img = self.dataset1_path / split / 'images' / img['file_name']\n",
    "            dst_img = self.output_path / split / 'images' / img['file_name']\n",
    "            if src_img.exists():\n",
    "                shutil.copy2(src_img, dst_img)\n",
    "\n",
    "        # Add annotations for dataset 1\n",
    "        for ann in ann1.get('annotations', []):\n",
    "            if ann['image_id'] in img_id_mapping1:\n",
    "                new_ann = ann.copy()\n",
    "                new_ann['id'] = ann_id_counter\n",
    "                new_ann['image_id'] = img_id_mapping1[ann['image_id']]\n",
    "                # Map category ID to merged category system\n",
    "                if ann['category_id'] in cat_id_map1:\n",
    "                    new_ann['category_id'] = cat_id_map1[ann['category_id']]\n",
    "                    merged_ann['annotations'].append(new_ann)\n",
    "                    ann_id_counter += 1\n",
    "\n",
    "        # Process dataset 2\n",
    "        img_id_mapping2 = {}\n",
    "\n",
    "        for img in sampled_imgs2:\n",
    "            new_img_id = len(merged_ann['images']) + 1\n",
    "            img_id_mapping2[img['id']] = new_img_id\n",
    "\n",
    "            new_img = img.copy()\n",
    "            new_img['id'] = new_img_id\n",
    "            merged_ann['images'].append(new_img)\n",
    "\n",
    "            # Copy image file\n",
    "            src_img = self.dataset2_path / split / 'images' / img['file_name']\n",
    "            dst_img = self.output_path / split / 'images' / img['file_name']\n",
    "            if src_img.exists():\n",
    "                shutil.copy2(src_img, dst_img)\n",
    "\n",
    "        # Add annotations for dataset 2\n",
    "        for ann in ann2.get('annotations', []):\n",
    "            if ann['image_id'] in img_id_mapping2:\n",
    "                new_ann = ann.copy()\n",
    "                new_ann['id'] = ann_id_counter\n",
    "                new_ann['image_id'] = img_id_mapping2[ann['image_id']]\n",
    "                # Map category ID to merged category system\n",
    "                if ann['category_id'] in cat_id_map2:\n",
    "                    new_ann['category_id'] = cat_id_map2[ann['category_id']]\n",
    "                    merged_ann['annotations'].append(new_ann)\n",
    "                    ann_id_counter += 1\n",
    "\n",
    "        # Save merged annotation\n",
    "        output_ann_path = self.output_path / split / f'annotations_{split}.json'\n",
    "        with open(output_ann_path, 'w') as f:\n",
    "            json.dump(merged_ann, f)\n",
    "\n",
    "        logger.info(f\"{split} split merged: {len(merged_ann['images'])} images, {len(merged_ann['annotations'])} annotations\")\n",
    "\n",
    "    def _create_category_mapping(self, categories):\n",
    "        \"\"\"Create mapping from original category IDs to merged category IDs\"\"\"\n",
    "        cat_map = {}\n",
    "        for cat in categories:\n",
    "            # Find corresponding category in merged categories\n",
    "            for merged_cat in self.merged_categories:\n",
    "                if merged_cat['name'] == cat['name']:\n",
    "                    cat_map[cat['id']] = merged_cat['id']\n",
    "                    break\n",
    "        return cat_map\n",
    "\n",
    "    def _merge_categories(self, cats1, cats2):\n",
    "        \"\"\"Legacy method - kept for compatibility\"\"\"\n",
    "        return self._merge_categories_list(cats1 + cats2)\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"COCO Dataset for PyTorch\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split='train', transforms=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load annotations\n",
    "        ann_path = self.root_dir / split / f'annotations_{split}.json'\n",
    "        with open(ann_path, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "\n",
    "        self.images = self.coco_data['images']\n",
    "        self.annotations = self.coco_data['annotations']\n",
    "        self.categories = {cat['id']: cat['name'] for cat in self.coco_data['categories']}\n",
    "\n",
    "        # Create image_id to annotations mapping\n",
    "        self.img_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_to_anns:\n",
    "                self.img_to_anns[img_id] = []\n",
    "            self.img_to_anns[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_path = self.root_dir / self.split / 'images' / img_info['file_name']\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get annotations for this image\n",
    "        img_id = img_info['id']\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "\n",
    "        # Extract bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([img_id])\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            # Apply albumentations transforms\n",
    "            transformed = self.transforms(image=image, bboxes=boxes.numpy(), class_labels=labels.numpy())\n",
    "            image = transformed['image']\n",
    "            if 'bboxes' in transformed and len(transformed['bboxes']) > 0:\n",
    "                target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"Factory class to create different detection models\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_faster_rcnn(num_classes, backbone='resnet50'):\n",
    "        \"\"\"Create Faster R-CNN model\"\"\"\n",
    "        if backbone == 'resnet34':\n",
    "            # Custom Faster R-CNN with ResNet34 backbone\n",
    "            from torchvision.models import resnet34\n",
    "            backbone_model = resnet34(pretrained=True)\n",
    "            backbone_model = nn.Sequential(*list(backbone_model.children())[:-2])\n",
    "            model = detection.FasterRCNN(backbone_model, num_classes=num_classes)\n",
    "        elif backbone == 'resnet50':\n",
    "            model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "            in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "            model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def create_yolov8_alternative(num_classes):\n",
    "        \"\"\"Create YOLOv8-like model using torchvision components\"\"\"\n",
    "        # Since YOLOv8 isn't directly available in torchvision, we'll use RetinaNet as alternative\n",
    "        model = detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "        num_anchors = model.head.classification_head.num_anchors\n",
    "        model.head.classification_head.num_classes = num_classes\n",
    "\n",
    "        # Reinitialize the classification head\n",
    "        model.head.classification_head.cls_logits = nn.Conv2d(\n",
    "            256, num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def create_detr_alternative(num_classes, backbone='resnet50'):\n",
    "        \"\"\"Create DETR-like model using available components\"\"\"\n",
    "        # Since DETR isn't directly available, we'll use SSD as an alternative\n",
    "        if backbone == 'resnet50':\n",
    "            # Use MobileNetV3 SSD as it's available in torchvision\n",
    "            model = detection.ssd300_vgg16(pretrained=True)\n",
    "            # Adjust for number of classes\n",
    "            model.head.classification_head.num_classes = num_classes\n",
    "        else:\n",
    "            # Fallback to Faster R-CNN for other backbones\n",
    "            model = ModelFactory.create_faster_rcnn(num_classes, backbone)\n",
    "\n",
    "        return model\n",
    "\n",
    "class MultiModelTrainer:\n",
    "    \"\"\"Train multiple detection models\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, num_classes=None, device='cuda'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.device = device if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # If num_classes not provided, infer from dataset\n",
    "        if num_classes is None:\n",
    "            self.num_classes = self._infer_num_classes()\n",
    "        else:\n",
    "            self.num_classes = num_classes\n",
    "\n",
    "        logger.info(f\"Training with {self.num_classes} classes (including background)\")\n",
    "\n",
    "        # Define transforms\n",
    "        self.train_transforms = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "        self.val_transforms = A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "    def _infer_num_classes(self):\n",
    "        \"\"\"Infer number of classes from dataset annotations\"\"\"\n",
    "        max_class_id = 0\n",
    "\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            ann_path = Path(self.dataset_path) / split / f'annotations_{split}.json'\n",
    "            if ann_path.exists():\n",
    "                with open(ann_path, 'r') as f:\n",
    "                    ann_data = json.load(f)\n",
    "\n",
    "                if 'categories' in ann_data and ann_data['categories']:\n",
    "                    # Get maximum category ID\n",
    "                    for cat in ann_data['categories']:\n",
    "                        max_class_id = max(max_class_id, cat['id'])\n",
    "                    break\n",
    "\n",
    "                # Fallback: check annotations for max category_id\n",
    "                if 'annotations' in ann_data:\n",
    "                    for ann in ann_data['annotations']:\n",
    "                        max_class_id = max(max_class_id, ann['category_id'])\n",
    "\n",
    "        # Add 1 for background class\n",
    "        return max_class_id + 1\n",
    "\n",
    "    def create_data_loaders(self, batch_size=4):\n",
    "        \"\"\"Create data loaders\"\"\"\n",
    "        train_dataset = COCODataset(self.dataset_path, 'train', self.train_transforms)\n",
    "        val_dataset = COCODataset(self.dataset_path, 'val', self.val_transforms)\n",
    "\n",
    "        def collate_fn(batch):\n",
    "            return tuple(zip(*batch))\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True,\n",
    "            collate_fn=collate_fn, num_workers=2\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=batch_size, shuffle=False,\n",
    "            collate_fn=collate_fn, num_workers=2\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train_model(self, model, train_loader, val_loader, epochs=10, lr=0.001):\n",
    "        \"\"\"Train a single model\"\"\"\n",
    "        model.to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "\n",
    "            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "            for images, targets in train_pbar:\n",
    "                images = [img.to(self.device) for img in images]\n",
    "                targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += losses.item()\n",
    "                train_pbar.set_postfix({'loss': losses.item()})\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n",
    "                for images, targets in val_pbar:\n",
    "                    images = [img.to(self.device) for img in images]\n",
    "                    targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                    loss_dict = model(images, targets)\n",
    "                    losses = sum(loss for loss in loss_dict.values())\n",
    "                    val_loss += losses.item()\n",
    "                    val_pbar.set_postfix({'loss': losses.item()})\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            logger.info(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "            # Save best model\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), f'best_model_epoch_{epoch+1}.pth')\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train_all_models(self, epochs=10, batch_size=4):\n",
    "        \"\"\"Train all model configurations\"\"\"\n",
    "        # Create data loaders\n",
    "        train_loader, val_loader = self.create_data_loaders(batch_size)\n",
    "\n",
    "        # Model configurations\n",
    "        configs = [\n",
    "            ('faster_rcnn', 'resnet34'),\n",
    "            ('faster_rcnn', 'resnet50'),\n",
    "            ('yolov8_alt', 'resnet50'),  # Using RetinaNet as alternative\n",
    "            ('detr_alt', 'resnet50'),    # Using SSD as alternative\n",
    "        ]\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for model_type, backbone in configs:\n",
    "            logger.info(f\"Training {model_type} with {backbone} backbone...\")\n",
    "\n",
    "            try:\n",
    "                # Create model\n",
    "                if model_type == 'faster_rcnn':\n",
    "                    model = ModelFactory.create_faster_rcnn(self.num_classes, backbone)\n",
    "                elif model_type == 'yolov8_alt':\n",
    "                    model = ModelFactory.create_yolov8_alternative(self.num_classes)\n",
    "                elif model_type == 'detr_alt':\n",
    "                    model = ModelFactory.create_detr_alternative(self.num_classes, backbone)\n",
    "\n",
    "                # Train model\n",
    "                trained_model = self.train_model(model, train_loader, val_loader, epochs)\n",
    "\n",
    "                # Save final model\n",
    "                model_name = f'{model_type}_{backbone}'\n",
    "                torch.save(trained_model.state_dict(), f'{model_name}_final.pth')\n",
    "                results[model_name] = trained_model\n",
    "\n",
    "                logger.info(f\"Completed training {model_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training {model_type} with {backbone}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Configuration\n",
    "    DATASET1_PATH = '/Volumes/Cucumber/Projects/datasets/sku110k'  # Update with actual path\n",
    "    DATASET2_PATH = '/Volumes/Cucumber/Projects/datasets/hcmus-iid'  # Update with actual path\n",
    "    OUTPUT_PATH = '/Volumes/Cucumber/Projects/datasets/merged-dataset'\n",
    "    WEIGHT1 = 0.8  # Weight for dataset 1\n",
    "    WEIGHT2 = 0.2  # Weight for dataset 2\n",
    "    EPOCHS = 5  # Reduced for Kaggle time limits\n",
    "    BATCH_SIZE = 4  # Reduced for memory constraints\n",
    "\n",
    "    # Step 1: Merge datasets and get number of classes\n",
    "    logger.info(\"Step 1: Merging datasets...\")\n",
    "    merger = COCODatasetMerger(DATASET1_PATH, DATASET2_PATH, OUTPUT_PATH, WEIGHT1, WEIGHT2)\n",
    "    num_classes, categories = merger.merge_datasets()\n",
    "\n",
    "    logger.info(f\"Dataset merged successfully!\")\n",
    "    logger.info(f\"Number of classes: {num_classes}\")\n",
    "    logger.info(f\"Categories: {[cat['name'] for cat in categories]}\")\n",
    "\n",
    "    # Step 2: Train models with inferred number of classes\n",
    "    logger.info(\"Step 2: Training models...\")\n",
    "    trainer = MultiModelTrainer(OUTPUT_PATH, num_classes)\n",
    "    results = trainer.train_all_models(epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Step 3: Summary\n",
    "    logger.info(\"Training completed!\")\n",
    "    logger.info(f\"Trained models: {list(results.keys())}\")\n",
    "    logger.info(f\"Final number of classes used: {num_classes}\")\n",
    "\n",
    "    return results, num_classes, categories\n",
    "\n",
    "# Example usage for Kaggle\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages (uncomment if needed)\n",
    "    # !pip install albumentations\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    # Run main pipeline\n",
    "    results, num_classes, categories = main()\n",
    "\n",
    "    print(\"Pipeline completed successfully!\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Categories found: {[cat['name'] for cat in categories]}\")\n",
    "    print(f\"Available models: {list(results.keys()) if results else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Additional utility functions for evaluation and inference\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"Evaluate a trained model\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(test_loader, desc='Evaluating'):\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            predictions.extend(outputs)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def visualize_predictions(model, dataset, device='cuda', num_samples=5):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image, target = dataset[i]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model([image.to(device)])\n",
    "\n",
    "        # Convert image for visualization\n",
    "        img_np = image.permute(1, 2, 0).numpy()\n",
    "        img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "        axes[i].imshow(img_np)\n",
    "        axes[i].set_title(f'Sample {i+1}')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "        scores = prediction[0]['scores'].cpu().numpy()\n",
    "\n",
    "        for box, score in zip(boxes, scores):\n",
    "            if score > 0.5:  # Confidence threshold\n",
    "                x1, y1, x2, y2 = box\n",
    "                axes[i].add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                              fill=False, color='red', linewidth=2))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('predictions_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Configuration helper\n",
    "def create_kaggle_config():\n",
    "    \"\"\"Create configuration for Kaggle environment\"\"\"\n",
    "    config = {\n",
    "        'dataset1_path': '/kaggle/input/dataset1',\n",
    "        'dataset2_path': '/kaggle/input/dataset2',\n",
    "        'output_path': '/kaggle/working/merged_dataset',\n",
    "        'weight1': 0.6,\n",
    "        'weight2': 0.4,\n",
    "        'epochs': 5,\n",
    "        'batch_size': 2,\n",
    "        'learning_rate': 0.001,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def get_dataset_info(dataset_path):\n",
    "    \"\"\"Get information about a COCO dataset\"\"\"\n",
    "    info = {\n",
    "        'num_classes': 0,\n",
    "        'categories': [],\n",
    "        'splits': {},\n",
    "        'total_images': 0,\n",
    "        'total_annotations': 0\n",
    "    }\n",
    "\n",
    "    dataset_path = Path(dataset_path)\n",
    "\n",
    "    # Check each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        ann_path = dataset_path / split / f'annotations_{split}.json'\n",
    "        if ann_path.exists():\n",
    "            with open(ann_path, 'r') as f:\n",
    "                ann_data = json.load(f)\n",
    "\n",
    "            split_info = {\n",
    "                'images': len(ann_data.get('images', [])),\n",
    "                'annotations': len(ann_data.get('annotations', [])),\n",
    "                'categories': len(ann_data.get('categories', []))\n",
    "            }\n",
    "            info['splits'][split] = split_info\n",
    "            info['total_images'] += split_info['images']\n",
    "            info['total_annotations'] += split_info['annotations']\n",
    "\n",
    "            # Get categories (use first available split)\n",
    "            if not info['categories'] and ann_data.get('categories'):\n",
    "                info['categories'] = ann_data['categories']\n",
    "                info['num_classes'] = len(ann_data['categories']) + 1  # +1 for background\n",
    "\n",
    "    return info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
